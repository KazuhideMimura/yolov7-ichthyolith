{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb2d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from os.path import join, basename, dirname\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6fa4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subdirs(path):\n",
    "    subdirs = glob(os.path.join(path, '*'))\n",
    "    subdirs = [s for s in subdirs if os.path.isdir(s)]\n",
    "    return(sorted(subdirs))\n",
    "\n",
    "def path_check(path, false = 'assert', return_flag = False):\n",
    "    flag = True\n",
    "    if not os.path.exists(path):\n",
    "        text = f\"path not exists: {path}\"\n",
    "        flag = False\n",
    "        if false == 'assert':\n",
    "            assert False, text\n",
    "        elif false == 'print':\n",
    "            print(text)\n",
    "    if return_flag:\n",
    "        return(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609dc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDD_path1 = 'D:/' # dataset\n",
    "HDD_path2 = 'E:/' # original image\n",
    "\n",
    "path_check(f\"{HDD_path1}dataset_for_yolo\")\n",
    "path_check(f\"{HDD_path2}Site_1149\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8af46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate new folder\n",
    "new_dataset_dir = f\"{HDD_path1}dataset_for_yolo/20221224_U1366_from_detection\"\n",
    "for folder in ['images', 'labels']:\n",
    "    os.makedirs(join(new_dataset_dir, '00_original', folder), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6d264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(img_name, detection_dir_site, new_dataset_dir, site):    \n",
    "    \n",
    "    # 情報の取得\n",
    "    img_name_sep = img_name.split('_')\n",
    "    sample = '_'.join(img_name_sep[:-3])\n",
    "    slide = '_'.join(img_name_sep[:-2])\n",
    "    img_path = join(img_dir_site, sample, slide, img_name)\n",
    "    label_name = img_name[:-4] + '.txt'\n",
    "    label_path = join(detection_dir_site, sample, slide, 'labels', label_name)\n",
    "    \n",
    "    # ラベルファイルをデータセットに組み込むかの判定\n",
    "    move = True\n",
    "    label_name = basename(label_path)\n",
    "    img_name = label_name[:-4] + '.jpg'\n",
    "    img_path = join(img_dir_site, sample, slide, img_name)\n",
    "    \n",
    "    # ラベル・画像のパスが存在しない場合は移動しない\n",
    "    if not all([path_check(label_path, false='print', return_flag=True), \n",
    "                 path_check(img_path, false='print', return_flag=True)]):\n",
    "        move = False\n",
    "    \n",
    "    # 以前のデータセットに含まれている場合は移動しない\n",
    "    if img_name in contained:\n",
    "        move = False\n",
    "    \n",
    "    # データセットの移動\n",
    "    if move:\n",
    "        new_img_path = join(new_dataset_dir, '00_original', 'images', img_name)\n",
    "        new_label_path = join(new_dataset_dir, '00_original', 'labels', label_name)\n",
    "        # 画像ファイルのコピー\n",
    "        shutil.copyfile(img_path, new_img_path)\n",
    "        \n",
    "        # ラベルファイルのコピー\n",
    "        # 信頼度を書き出している場合は、信頼度を除外する\n",
    "        with open(label_path, 'r') as f1:\n",
    "            lines = f1.readlines()\n",
    "        if len(lines[0].split()) == 6:\n",
    "            lines = [' '.join(l.split()[:-1]) for l in lines]        \n",
    "        with open(new_label_path, 'w') as f2:\n",
    "            f2.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13c470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12217 images\n"
     ]
    }
   ],
   "source": [
    "# 以前のデータセットにすでに入っている画像\n",
    "previous_dataset_dir = f\"{HDD_path1}dataset_for_yolo/20221209\"\n",
    "path_check(previous_dataset_dir)\n",
    "contained = set([basename(p) for p in\n",
    "                 glob(join(previous_dataset_dir, '02_all', '*', 'images', '*.jpg'))])\n",
    "print(len(contained), 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4da702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_samples = set(['576B_03_06_81', '576B_04_02_25', '576B_04_04_75', \n",
    "                       '576B_05_01_25', '576B_05_05_25'])\n",
    "learn_tooth_samples = set(['576B_01_02_77', '576B_01_06_52', '576B_02_03_125', \n",
    "                           '576B_02_07_23', '576B_03_03_125', \n",
    "                           '1149A_14_05_87', '1149A_18_04_38', '1149B_03_06_25', \n",
    "                           'U1366C_02_CC_11', 'U1366C_03_03_125'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19a72493",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = ['Site_U1366']\n",
    "class_names = ['tooth', 'denticle', 'sawToothed']\n",
    "\n",
    "for site in sites:\n",
    "    path_check(f\"{HDD_path2}{site}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b217668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "for site in sites:\n",
    "    \n",
    "    img_dir_site = f\"{HDD_path2}{site}\"\n",
    "    detection_dir_site = f\"../runs/detect/v7x_20221209_3cls_all/{site}\"\n",
    "\n",
    "    for p in [img_dir_site, detection_dir_site, previous_dataset_dir]:\n",
    "        path_check(p)\n",
    "    \n",
    "    samples = [basename(p) for p in get_subdirs(img_dir_site)]\n",
    "    samples = [s for s in samples if s not in exclude_samples]\n",
    "    samples.sort()\n",
    "\n",
    "    for sample in samples:\n",
    "        detection_csv = join(detection_dir_site, 'cropped_images', \n",
    "                             sample, 'detections.csv')\n",
    "        path_check(detection_csv)\n",
    "        df = pd.read_csv(detection_csv, index_col=0)\n",
    "        if sample not in learn_tooth_samples:\n",
    "            df = df[df['class_no'].isin([1, 2])]\n",
    "        img_names = set(df['filename'])\n",
    "\n",
    "        for img_name in img_names:\n",
    "            process_img(img_name, detection_dir_site, new_dataset_dir, site)\n",
    "    \n",
    "class_label_path = join(new_dataset_dir, '00_original', 'labels', 'classes.txt')\n",
    "with open(class_label_path, 'w') as f3:\n",
    "    f3.write('\\n'.join(class_names))\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20b134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
